{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1473d5-d4a1-4139-9a91-0c9ba146f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6d86eae1aa47c9a7dc3ed11261abf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_331/641727066.py:98: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chatbox\", elem_classes=\"control-height\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://9bf2429760b6d367c0.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9bf2429760b6d367c0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping data after last boundary\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from threading import Thread\n",
    "import io\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "# --- Model and Tokenizer Configuration ---\n",
    "CHECKPOINT_PATH = \"Qwen2.5-7B-Instruct-1M\"\n",
    "\n",
    "def load_model_tokenizer():\n",
    "    \"\"\"Loads the pre-trained model and tokenizer.\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        resume_download=True,\n",
    "    )\n",
    "\n",
    "    device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=device_map,\n",
    "        resume_download=True,\n",
    "    ).eval()\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# --- Core Chat Logic ---\n",
    "def chat_stream(model, tokenizer, query, history):\n",
    "    \"\"\"Generates a response from the model in a streaming fashion.\"\"\"\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if assistant_msg is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # We need a TextIteratorStreamer for this to work\n",
    "    from transformers import TextIteratorStreamer\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer, skip_prompt=True, timeout=60.0, skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        **model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    for new_text in streamer:\n",
    "        yield new_text\n",
    "\n",
    "# --- Main Application UI and Logic ---\n",
    "def build_chatbot_ui(model, tokenizer):\n",
    "    \"\"\"Builds the Gradio web interface for the chatbot.\"\"\"\n",
    "    \n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\".control-height { height: 500px; overflow: auto; }\"\n",
    "    ) as demo:\n",
    "        df_state = gr.State(None)\n",
    "        task_history = gr.State([])\n",
    "\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center;\">\n",
    "                <h1>ü§ñ Conversational AI Data Analyst</h1>\n",
    "                <p>Upload a CSV, ask a question, and the AI will analyze the data and explain the results conversationally.</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                file_uploader = gr.File(label=\"Upload your CSV\", file_types=[\".csv\"])\n",
    "            \n",
    "            with gr.Column(scale=2):\n",
    "                chatbot = gr.Chatbot(label=\"Chatbox\", elem_classes=\"control-height\")\n",
    "                query_box = gr.Textbox(lines=3, label=\"Your Question\", placeholder=\"e.g., What percentage of conversations were on each topic?\")\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "                    regenerate_btn = gr.Button(\"Regenerate\")\n",
    "                    clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        def load_csv_data(file, chatbot_history):\n",
    "            if file is not None:\n",
    "                try:\n",
    "                    df = pd.read_csv(file.name)\n",
    "                    chatbot_history.append((\"‚úÖ File loaded successfully.\", \"What would you like to know about the data?\"))\n",
    "                    return df, chatbot_history\n",
    "                except Exception as e:\n",
    "                    return None, chatbot_history + [(\"\", f\"‚ùå Error: {e}\")]\n",
    "            return None, chatbot_history\n",
    "\n",
    "        def predict(query, chatbot_history, df, task_history_state):\n",
    "            if df is None:\n",
    "                chatbot_history.append((query, \"Please upload a CSV file first.\"))\n",
    "                yield chatbot_history\n",
    "                return\n",
    "\n",
    "            # --- Step 1: Generate and execute code to get the raw data ---\n",
    "            string_buffer = io.StringIO()\n",
    "            df.info(buf=string_buffer)\n",
    "            df_info = string_buffer.getvalue()\n",
    "\n",
    "            code_generation_prompt = f\"\"\"\n",
    "You are a data analysis AI. Based on the user's question and the DataFrame schema, write a Python script using pandas to get the data needed to answer the question. The DataFrame is named `df`.\n",
    "Your script must print the result. Provide only raw Python code.\n",
    "\n",
    "**Schema (df.info()):**\n",
    "{df_info}\n",
    "**User Question:** \"{query}\"\n",
    "\"\"\"\n",
    "            chatbot_history.append((query, \"ü§î Step 1: Analyzing data...\"))\n",
    "            yield chatbot_history\n",
    "\n",
    "            # Use a non-streaming call to get the code block\n",
    "            text = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": code_generation_prompt}], tokenize=False, add_generation_prompt=True)\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024, do_sample=False)\n",
    "            response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            generated_code = response_text.split(code_generation_prompt)[-1].strip()\n",
    "            if \"```python\" in generated_code:\n",
    "                generated_code = generated_code.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "\n",
    "            # Execute the code\n",
    "            old_stdout = sys.stdout\n",
    "            redirected_output = sys.stdout = StringIO()\n",
    "            try:\n",
    "                exec(generated_code, {'df': df, 'pd': pd})\n",
    "                sys.stdout = old_stdout\n",
    "                raw_result = redirected_output.getvalue()\n",
    "\n",
    "                # --- Step 2: Summarize the result conversationally ---\n",
    "                chatbot_history[-1] = (query, \"‚úçÔ∏è Step 2: Summarizing results...\")\n",
    "                yield chatbot_history\n",
    "\n",
    "                if not raw_result:\n",
    "                    summary = \"The analysis ran successfully but produced no specific data to summarize.\"\n",
    "                else:\n",
    "                    summarization_prompt = f\"\"\"\n",
    "You are a helpful AI assistant. Your task is to explain the following data to a user in a clear, conversational way.\n",
    "The user originally asked: \"{query}\"\n",
    "Summarize the key findings from the data below to answer their question. Don't just list the numbers; explain what they mean.\n",
    "\n",
    "**Data to Summarize:**\n",
    "{raw_result}\n",
    "\"\"\"\n",
    "                    # Use the streaming function for the final conversational answer\n",
    "                    final_summary = \"\"\n",
    "                    for new_text in chat_stream(model, tokenizer, summarization_prompt, history=[]):\n",
    "                        final_summary += new_text\n",
    "                        chatbot_history[-1] = (query, final_summary)\n",
    "                        yield chatbot_history\n",
    "                    task_history_state.append((query, final_summary))\n",
    "                    return\n",
    "\n",
    "            except Exception as e:\n",
    "                sys.stdout = old_stdout\n",
    "                error_message = f\"‚ùå **Error during code execution:**\\n\\n```\\n{str(e)}\\n```\"\n",
    "                chatbot_history[-1] = (query, error_message)\n",
    "                task_history_state.append((query, error_message))\n",
    "                yield chatbot_history\n",
    "\n",
    "        def regenerate(chatbot_history, task_history_state, df):\n",
    "            if not task_history_state:\n",
    "                yield chatbot_history\n",
    "                return\n",
    "            last_query, _ = task_history_state.pop(-1)\n",
    "            chatbot_history.pop(-1)\n",
    "            yield from predict(last_query, chatbot_history, df, task_history_state)\n",
    "\n",
    "        def clear_history():\n",
    "            return [], None, []\n",
    "\n",
    "        def reset_user_input():\n",
    "            return gr.update(value=\"\")\n",
    "\n",
    "        # Wire up components\n",
    "        file_uploader.upload(load_csv_data, [file_uploader, chatbot], [df_state, chatbot])\n",
    "        submit_btn.click(predict, [query_box, chatbot, df_state, task_history], [chatbot]).then(reset_user_input, [], [query_box])\n",
    "        regenerate_btn.click(regenerate, [chatbot, task_history, df_state], [chatbot])\n",
    "        clear_btn.click(clear_history, [], [chatbot, df_state, task_history])\n",
    "        \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    model, tokenizer = load_model_tokenizer()\n",
    "    demo = build_chatbot_ui(model, tokenizer)\n",
    "    demo.queue().launch(share=True, inbrowser=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e2a22-420d-4198-83cc-258a9c2ffc87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
