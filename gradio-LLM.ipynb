{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061fc99-a517-45c6-8015-57433a364c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import io\n",
    "\n",
    "# --- Model and Tokenizer Configuration ---\n",
    "# We are defining the model path directly here for simplicity.\n",
    "# This model is powerful but requires significant resources (GPU recommended).\n",
    "CHECKPOINT_PATH = \"Qwen2.5-7B-Instruct-1M\"\n",
    "  \n",
    "def load_model_tokenizer():\n",
    "    \"\"\"Loads the pre-trained model and tokenizer.\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        resume_download=True,\n",
    "    )\n",
    "\n",
    "    # Use GPU if available, otherwise fall back to CPU.\n",
    "    #device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device_map = \"cpu\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=device_map,\n",
    "        resume_download=True,\n",
    "    ).eval()\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# --- Core Chat Logic ---\n",
    "def chat_stream(model, tokenizer, query, history):\n",
    "    \"\"\"Generates a response from the model in a streaming fashion.\"\"\"\n",
    "    # Apply the chat template to format the conversation history and new query\n",
    "    messages = []\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        if assistant_msg is not None:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Setup the streamer for text generation\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer, skip_prompt=True, timeout=60.0, skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    generation_kwargs = {\n",
    "        **model_inputs,\n",
    "        \"streamer\": streamer,\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "    \n",
    "    # Run generation in a separate thread\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Yield new text as it becomes available\n",
    "    for new_text in streamer:\n",
    "        yield new_text\n",
    "\n",
    "# --- Main Application UI and Logic ---\n",
    "def build_chatbot_ui(model, tokenizer):\n",
    "    \"\"\"Builds the Gradio web interface for the chatbot.\"\"\"\n",
    "    \n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\".control-height { height: 500px; overflow: auto; }\"\n",
    "    ) as demo:\n",
    "        # State management\n",
    "        df_state = gr.State(None) # To hold the pandas DataFrame\n",
    "        task_history = gr.State([]) # To hold the conversation history\n",
    "\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            <div style=\"text-align: center;\">\n",
    "                <h1>ðŸ“Š dennislee CSV Analysis Chatbot</h1>\n",
    "                <p>Upload a CSV file, and then ask questions about its content. The model will analyze the data summary to provide answers.</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                # File Uploader and DataFrame Display\n",
    "                file_uploader = gr.File(\n",
    "                    label=\"Upload your CSV\",\n",
    "                    file_types=[\".csv\"],\n",
    "                    elem_id=\"file_uploader\"\n",
    "                )\n",
    "                df_output = gr.DataFrame(\n",
    "                    label=\"DataFrame Head\",\n",
    "                    headers=None,\n",
    "                    wrap=True,\n",
    "                    row_count=5, # Changed max_rows to row_count to fix the TypeError\n",
    "                )\n",
    "            \n",
    "            with gr.Column(scale=2):\n",
    "                # Chatbot Interface\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Chatbox\",\n",
    "                    elem_classes=\"control-height\"\n",
    "                )\n",
    "                query_box = gr.Textbox(\n",
    "                    lines=3,\n",
    "                    label=\"Your Question\",\n",
    "                    placeholder=\"e.g., How many rows are there? or What is the average value in the 'sales' column?\"\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "                    regenerate_btn = gr.Button(\"Regenerate\")\n",
    "                    clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        # --- Event Handlers ---\n",
    "\n",
    "        def load_csv_data(file, chatbot_history):\n",
    "            \"\"\"\n",
    "            Handles the CSV file upload. It reads the CSV into a pandas DataFrame,\n",
    "            stores it in the state, and displays its head in the UI.\n",
    "            \"\"\"\n",
    "            if file is not None:\n",
    "                try:\n",
    "                    df = pd.read_csv(file.name)\n",
    "                    chatbot_history.append((\n",
    "                        f\"Successfully loaded `{file.name}`.\",\n",
    "                        \"I'm ready for your questions about the data.\"\n",
    "                    ))\n",
    "                    return df, df.head(), chatbot_history\n",
    "                except Exception as e:\n",
    "                    error_message = f\"Error loading CSV: {e}\"\n",
    "                    chatbot_history.append((f\"Failed to load `{file.name}`.\", error_message))\n",
    "                    return None, None, chatbot_history\n",
    "            return None, None, chatbot_history\n",
    "\n",
    "        def predict(query, chatbot_history, df):\n",
    "            \"\"\"\n",
    "            Handles the user's query. It provides the model with context about\n",
    "            the DataFrame before asking for a response.\n",
    "            \"\"\"\n",
    "            if df is None:\n",
    "                chatbot_history.append((query, \"Please upload a CSV file first so I can answer your questions about it.\"))\n",
    "                yield chatbot_history\n",
    "                return\n",
    "\n",
    "            # Create a context string with DataFrame info\n",
    "            string_buffer = io.StringIO()\n",
    "            df.info(buf=string_buffer)\n",
    "            df_info = string_buffer.getvalue()\n",
    "            \n",
    "            contextual_query = f\"\"\"\n",
    "You are an intelligent data analyst. Based on the following summary of a pandas DataFrame, please answer the user's question.\n",
    "\n",
    "**DataFrame Head:**\n",
    "```\n",
    "{df.head().to_string()}\n",
    "```\n",
    "\n",
    "**DataFrame Info:**\n",
    "```\n",
    "{df_info}\n",
    "```\n",
    "\n",
    "---\n",
    "**User Question:** {query}\n",
    "\"\"\"\n",
    "            chatbot_history.append((query, \"\"))\n",
    "            full_response = \"\"\n",
    "            for new_text in chat_stream(model, tokenizer, contextual_query, history=task_history.value):\n",
    "                full_response += new_text\n",
    "                chatbot_history[-1] = (query, full_response)\n",
    "                yield chatbot_history\n",
    "            \n",
    "            # Update the task history after the full response is generated\n",
    "            task_history.value.append((query, full_response))\n",
    "\n",
    "        def regenerate(chatbot_history, task_history_state, df):\n",
    "            \"\"\"Regenerates the last response.\"\"\"\n",
    "            if not task_history_state:\n",
    "                yield chatbot_history\n",
    "                return\n",
    "            \n",
    "            last_item = task_history_state.pop(-1)\n",
    "            chatbot_history.pop(-1)\n",
    "            \n",
    "            yield from predict(last_item[0], chatbot_history, df)\n",
    "\n",
    "        def clear_history(chatbot, df_state, task_history_state, df_output):\n",
    "            \"\"\"Clears the chat history, DataFrame state, and UI.\"\"\"\n",
    "            task_history_state.clear()\n",
    "            chatbot.clear()\n",
    "            # Garbage collect to free up memory\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            return [], None, [], None\n",
    "\n",
    "        def reset_user_input():\n",
    "            \"\"\"Clears the user input box.\"\"\"\n",
    "            return gr.update(value=\"\")\n",
    "\n",
    "        # Wire up the components to the functions\n",
    "        file_uploader.upload(\n",
    "            load_csv_data,\n",
    "            inputs=[file_uploader, chatbot],\n",
    "            outputs=[df_state, df_output, chatbot]\n",
    "        )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            predict,\n",
    "            inputs=[query_box, chatbot, df_state],\n",
    "            outputs=[chatbot]\n",
    "        ).then(reset_user_input, [], [query_box])\n",
    "\n",
    "        regenerate_btn.click(\n",
    "            regenerate,\n",
    "            inputs=[chatbot, task_history, df_state],\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            clear_history,\n",
    "            inputs=[chatbot, df_state, task_history, df_output],\n",
    "            outputs=[chatbot, df_state, task_history, df_output]\n",
    "        )\n",
    "        \n",
    "    return demo\n",
    "\n",
    "def main():\n",
    "    \"\"\"The main function to load the model and launch the demo.\"\"\"\n",
    "    model, tokenizer = load_model_tokenizer()\n",
    "    demo = build_chatbot_ui(model, tokenizer)\n",
    "    \n",
    "    # Launch the Gradio app\n",
    "    demo.queue().launch(\n",
    "        share=True,\n",
    "        inbrowser=True,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
